## ai の基本的な考え方

`ai` は、現在のセッションコンテキストとユーザーからのメッセージをもとに、LLM へ問い合わせを行うコマンドです。

```bash
ai [options] [task] [message...]
```

- **通常利用**では `task` を省略し、`message` に自然言語で依頼内容を書きます。
- `aish` 上で `ai` を実行すると、`AISH_SESSION` に紐づくセッション内容（最近の入出力など）がコンテキストとして利用されます。
- `task` を指定すると、対応するタスクスクリプトが見つかった場合は **LLM ではなくスクリプトが実行される** 挙動になります（タスク機能の詳細は後回しにしています）。

### 典型的な利用イメージ

```bash
(aish:1.2K)$ ai テストが失敗している原因と、次に確認すべき点を教えて
```

- 直前までのコンソール出力（テストのエラーなど）を踏まえて、失敗理由の推測や次のアクション案を提案させる、といった使い方を想定しています。

## 基本的な使い方

### シンプルな質問

`aish` か通常のシェル上から、単発の質問を投げる場合:

```bash
ai Rust の Result と Option の違いを簡潔に説明してください
```

- この場合、特にセッションコンテキストが重要でなければ、普通の LLM に対して1回だけ問合せるのと同様の挙動になります。

### aish セッションと組み合わせて使う

`aish` 上で作業しているときに `ai` を呼ぶと、そのセッション内の履歴がコンテキストに加わる点が特徴です。

```bash
# aish セッション内でテストを実行
(aish:0)$ cargo test
...（エラー出力）...

# そのまま ai に相談
(aish:1.5K)$ ai このテストエラーの原因を推測し、修正方針を提案してください
```

- `ai` は、セッションディレクトリ（`AISH_SESSION`）から履歴を読み込み、LLM に送るべきコンテキストを選択します。
- どの程度の履歴が送られるかは、内部のコンテキスト戦略（`context_budget` など）に依存します。

> 注意: 非常に長いログや大量の履歴がある場合、すべてがそのまま送信されるわけではありません。  
> 実装側で要約・絞り込みを行い、LLM のトークン制限内に収まるよう調整します。

## オプションと設定のポイント

`ai --help` に表示される主なオプションのうち、よく使うものを抜粋して解説します。

### プロファイルとモデル

- `-p, --profile <profile>`  
  - LLM プロバイダや既定のモデル・設定をまとめた「プロファイル」を指定します。  
  - 例: `gemini`, `gpt`, `echo` など。
  - 実際の定義は `profiles.json` に保存されており、`-L, --list-profiles` で一覧表示できます。

- `-m, --model <model>`  
  - 個別のモデル名を直接指定します。
  - 例: `gemini-2.0`, `gpt-4` など。
  - プロファイルのデフォルトモデルを一時的に上書きしたいときに使用します。

### システムインストラクション

- `-S, --system <instruction>`  
  - その問い合わせに対するシステムインストラクション（ロール・制約など）を直接指定します。
  - 例:

    ```bash
    ai -S 'あなたは Rust のシニアエンジニアです' 'このコードのリファクタ案を出して'
    ```

- 省略した場合: system instruction は送られません（明示的に `-S` で指定した場合のみ使用されます）。

### 対話制御・ツール利用まわり

- `-c, --continue`  
  - 直前のエージェントループの状態から再開します。  
    - 例: ツール実行回数の上限に達したあと、続きの対話を行いたい場合。
  - `AISH_SESSION` が正しく設定されている必要があります（通常は `aish` から呼べば自動で設定されます）。

- `--no-interactive`  
  - CI など、人間の確認を挟みたくない環境向けのオプションです。
  - ツール実行の承認や `continue` の確認などを自動で拒否する挙動になります。

- `--list-tools`  
  - 現在のプロファイルで有効なツール一覧を表示します。  
  - 例: `ai -p echo --list-tools`

### デバッグ・補完

- `-v, --verbose`  
  - 詳細なデバッグログを標準エラーに出力します。問題の切り分けや、どのようなコンテキストが構築されているかの把握に役立ちます。

- `--generate <shell>`  
  - bash / zsh / fish 用の補完スクリプトを生成します。生成したスクリプトをシェルに読み込むと、**TAB キー**で以下を補完できます。  
    - タスク名（`ai ` の直後に TAB）  
    - `-p` / `--profile` のプロファイル名  
    - `-M` / `--mode` のモード名  
    - オプション名（`-h`, `-c`, `-v` など）  
  - 例:

    ```bash
    ai --generate bash > ~/.config/aish/ai-completion.bash
    source ~/.config/aish/ai-completion.bash
    ```

## プロンプト設計のコツ（概要）

詳細なシナリオは後回しにする前提で、ここでは最低限の指針のみ示します。

- **やってほしいことを一文で先に書く**  
  - 「何を説明させたいか」「何を決めたいか」を冒頭で明示します。
- **前提条件や制約は箇条書きにする**  
  - 例: 「このプロジェクトは Rust 1.80 以上」「外部 API の仕様は変更できない」など。
- **出力フォーマットをはっきり指定する**  
  - 箇条書き・コードブロック・表形式など、欲しい形式を具体的に書きます。

例:

```bash
ai '次の条件でリファクタリング案を出してください。
- 条件: Rust 1.80 以上、外部 API は変更禁止
- 目的: 可読性を上げつつ、パフォーマンスを悪化させないこと
- 出力形式: 箇条書き + 必要ならコード例'
```

## セキュリティ・プライバシー上の注意（ai 視点の概要）

- `ai` は、セッションディレクトリ内の情報（ログ・履歴など）と、ユーザーのプロンプトを LLM に送信します。
- セッション内に機密情報（秘密鍵・アクセストークン・個人情報など）が含まれている場合、それらが **LLM プロバイダに送信される可能性** があることに注意してください。
- `leakscan` などを併用することで、危険な文字列を事前に検出することはできますが、**完全な保証ではありません**。

運用上の推奨:

- 公開してはいけない情報が含まれるログやファイルは、`ai` にそのまま渡さない / そもそも生成しないようにする。
- 必要に応じて、ログの一部をマスク・要約したうえでプロンプトに貼る。
- 重要なリポジトリでは、`leakscan` を CI などに統合し、人間の確認を挟む。

## まとめ

- `ai` は、`aish` セッションや各種設定を踏まえて LLM に問い合わせるためのフロントエンドです。
- 基本的な使い方は「自然言語で依頼を書く」だけですが、プロファイル・モデル・システムインストラクション・対話制御オプションを理解することで、より安定した結果を得られます。
- 今後、具体的なシナリオやワークフロー（コードレビュー、バグ調査、ドキュメント生成など）は、別セクションとしてこのドキュメント内に追加していく想定です。

